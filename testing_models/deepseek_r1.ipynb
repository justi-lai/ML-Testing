{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "95adef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from huggingface_hub import hf_hub_download\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "abc2c77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE(nn.Module):\n",
    "    def __init__(self, head_dim, max_tokens=4096, base=10000):\n",
    "        super(RoPE, self).__init__()\n",
    "        \n",
    "        assert head_dim % 2 == 0, \"head_dim must be even\"\n",
    "\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "        t = torch.arange(max_tokens, dtype=torch.float32)\n",
    "        freqs = torch.outer(t, inv_freq)\n",
    "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "\n",
    "        cos_cached = freqs_cis.real.repeat_interleave(2, dim=1)\n",
    "        sin_cached = freqs_cis.imag.repeat_interleave(2, dim=1)\n",
    "\n",
    "        self.register_buffer(\"cos_cached\", cos_cached, persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", sin_cached, persistent=False)\n",
    "\n",
    "    def _rotate_half(self, x):\n",
    "        x1 = x[..., 0::2]\n",
    "        x2 = x[..., 1::2]\n",
    "        return torch.stack((-x2, x1), dim=-1).flatten(-2)\n",
    "    \n",
    "    def forward(self, x, seq_len, offset=0):\n",
    "        cos = self.cos_cached[offset:offset + seq_len]\n",
    "        sin = self.sin_cached[offset:offset + seq_len]\n",
    "\n",
    "        cos = cos.unsqueeze(0).unsqueeze(1)\n",
    "        sin = sin.unsqueeze(0).unsqueeze(1)\n",
    "        return (x * cos) + (self._rotate_half(x) * sin)\n",
    "\n",
    "class MLAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ukv_dim, max_tokens=4096):\n",
    "        super(MLAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ukv_dim = ukv_dim\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.Wdkv = nn.Linear(embed_dim, ukv_dim, bias=False)\n",
    "        self.Wuk = nn.Linear(ukv_dim, embed_dim, bias=False)\n",
    "        self.Wuv = nn.Linear(ukv_dim, embed_dim, bias=False)\n",
    "        self.Wq = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.Wo = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "        self.rope = RoPE(self.head_dim, max_tokens)\n",
    "\n",
    "        self.kv_latent_cache = None\n",
    "        self.cache_pos = None\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def _init_cache(self, batch_size, device, dtype):\n",
    "        self.kv_latent_cache = torch.zeros(\n",
    "            (batch_size, self.max_tokens, self.ukv_dim),\n",
    "            device=device,\n",
    "            dtype=dtype\n",
    "        )\n",
    "        self.cache_pos = torch.zeros(batch_size, dtype=torch.long, device=device)\n",
    "\n",
    "    def forward(self, x, use_cache=True):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        q = self.Wq(x)\n",
    "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        latent_kv = self.Wdkv(x)\n",
    "\n",
    "        attn_mask = None\n",
    "        current_pos_offset = 0\n",
    "        if use_cache:\n",
    "            if self.kv_latent_cache is None or self.kv_latent_cache.shape[0] != batch_size:\n",
    "                self._init_cache(batch_size, x.device, x.dtype)\n",
    "            \n",
    "            current_pos_offset = self.cache_pos[0].item()\n",
    "\n",
    "            start_pos = self.cache_pos.view(-1, 1)\n",
    "            indices = start_pos + torch.arange(seq_len, device=x.device).view(1, -1)\n",
    "            if torch.any(indices >= self.max_tokens):\n",
    "                raise ValueError(\"Sequence length exceeds maximum tokens in cache.\")\n",
    "            \n",
    "            self.kv_latent_cache.scatter_(1, indices.unsqueeze(-1).expand(-1, -1, self.ukv_dim), latent_kv)\n",
    "            self.cache_pos += seq_len\n",
    "\n",
    "            max_pos = self.cache_pos.max().item()\n",
    "            full_latent_kv = self.kv_latent_cache[:, : max_pos, :]\n",
    "\n",
    "            kv_indices = torch.arange(max_pos, device=x.device).view(1, -1)\n",
    "            mask = kv_indices < self.cache_pos.view(-1, 1)\n",
    "            attn_mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        else:\n",
    "            full_latent_kv = latent_kv\n",
    "            attn_mask = None\n",
    "\n",
    "        attn_seq_len = full_latent_kv.shape[1]\n",
    "\n",
    "        k = self.Wuk(full_latent_kv)\n",
    "        v = self.Wuv(full_latent_kv)\n",
    "\n",
    "        k = k.view(batch_size, attn_seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, attn_seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        q = self.rope(q, seq_len, offset=current_pos_offset)\n",
    "        k = self.rope(k, attn_seq_len, offset=current_pos_offset)\n",
    "\n",
    "        attn_output = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "\n",
    "        output = self.Wo(attn_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def reset_cache(self):\n",
    "        if self.kv_latent_cache is not None:\n",
    "            self.kv_latent_cache.zero_()\n",
    "            \n",
    "        if self.cache_pos is not None:\n",
    "            self.cache_pos.zero_()\n",
    "    \n",
    "    def null_cache(self):\n",
    "        self.kv_latent_cache = None\n",
    "        self.cache_pos = None\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self._norm(x.float()).type_as(x) * self.weight\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.w_gate = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "        self.w_up = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "        self.w_down = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.w_down(F.silu(self.w_gate(x)) * self.w_up(x))\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MLAttention(\n",
    "            embed_dim=config['embed_dim'],\n",
    "            num_heads=config['num_heads'],\n",
    "            ukv_dim=config['ukv_dim'],\n",
    "            max_tokens=config['max_tokens']\n",
    "        )\n",
    "        self.feed_forward = FeedForward(\n",
    "            embed_dim=config['embed_dim'],\n",
    "            hidden_dim=config['ffn_hidden_dim']\n",
    "        )\n",
    "        self.attention_norm = RMSNorm(config['embed_dim'])\n",
    "        self.ffn_norm = RMSNorm(config['embed_dim'])\n",
    "    \n",
    "    def forward(self, x, use_cache=True):\n",
    "        h = x + self.attention(self.attention_norm(x), use_cache=use_cache)\n",
    "        out = h + self.feed_forward(self.ffn_norm(h))\n",
    "        return out\n",
    "    \n",
    "class DeepSeekModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(DeepSeekModel, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.tok_embeddings = nn.Embedding(config['vocab_size'], config['embed_dim'])\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerBlock(config) for _ in range(config['num_layers'])]\n",
    "        )\n",
    "\n",
    "        self.norm = RMSNorm(config['embed_dim'])\n",
    "\n",
    "        self.output_head = nn.Linear(config['embed_dim'], config['vocab_size'], bias=False)\n",
    "    \n",
    "    def forward(self, input_ids, use_cache=True):\n",
    "        x = self.tok_embeddings(input_ids)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, use_cache=use_cache)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        logits = self.output_head(x)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def reset_cache(self):\n",
    "        for layer in self.layers:\n",
    "            layer.attention.reset_cache()\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c122d562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Full Model Test ===\n",
      "Testing with Batch Size: 16\n",
      "Configuration: {'vocab_size': 10000, 'max_tokens': 4096, 'num_layers': 4, 'embed_dim': 256, 'num_heads': 4, 'ukv_dim': 64, 'ffn_hidden_dim': 1024}\n",
      "\n",
      "Model Architecture:\n",
      "DeepSeekModel(\n",
      "  (tok_embeddings): Embedding(10000, 256)\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x TransformerBlock(\n",
      "      (attention): MLAttention(\n",
      "        (Wdkv): Linear(in_features=256, out_features=64, bias=False)\n",
      "        (Wuk): Linear(in_features=64, out_features=256, bias=False)\n",
      "        (Wuv): Linear(in_features=64, out_features=256, bias=False)\n",
      "        (Wq): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (Wo): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (rope): RoPE()\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (w_gate): Linear(in_features=256, out_features=1024, bias=False)\n",
      "        (w_up): Linear(in_features=256, out_features=1024, bias=False)\n",
      "        (w_down): Linear(in_features=1024, out_features=256, bias=False)\n",
      "      )\n",
      "      (attention_norm): RMSNorm()\n",
      "      (ffn_norm): RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      "  (output_head): Linear(in_features=256, out_features=10000, bias=False)\n",
      ")\n",
      "\n",
      "Total Trainable Parameters: 8.99M\n",
      "\n",
      "=== Testing Training Pass ===\n",
      "Input IDs shape: torch.Size([4, 128])\n",
      "Output logits shape: torch.Size([4, 128, 10000])\n",
      "Training pass successful!\n",
      "\n",
      "=== Testing Inference Pass ===\n",
      "Step 1: Next token ID shape: torch.Size([16, 1])\n",
      "Step 2: Next token ID shape: torch.Size([16, 1])\n",
      "Step 3: Next token ID shape: torch.Size([16, 1])\n",
      "Step 4: Next token ID shape: torch.Size([16, 1])\n",
      "Step 5: Next token ID shape: torch.Size([16, 1])\n",
      "\n",
      "Final generated sequence length: 6\n",
      "Inference pass successful!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_config = {\n",
    "    'vocab_size': 10000,\n",
    "    'max_tokens': 4096,\n",
    "    'num_layers': 4,\n",
    "    'embed_dim': 256,\n",
    "    'num_heads': 4,\n",
    "    'ukv_dim': 64,\n",
    "    'ffn_hidden_dim': 256 * 4\n",
    "}\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "print(\"=== Full Model Test ===\")\n",
    "print(f\"Testing with Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Configuration: {model_config}\")\n",
    "\n",
    "model = DeepSeekModel.from_config(model_config).to(device)\n",
    "print(f\"\\nModel Architecture:\\n{model}\")\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal Trainable Parameters: {num_params / 1e6:.2f}M\")\n",
    "\n",
    "print(\"\\n=== Testing Training Pass ===\")\n",
    "model.train()\n",
    "model.reset_cache()\n",
    "dummy_ids = torch.randint(0, model_config['vocab_size'], (4, 128)).to(device)\n",
    "logits = model(dummy_ids, use_cache=False)\n",
    "print(f\"Input IDs shape: {dummy_ids.shape}\")\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "assert logits.shape == (4, 128, model_config['vocab_size']), \"Output shape mismatch\"\n",
    "print(\"Training pass successful!\")\n",
    "\n",
    "print(\"\\n=== Testing Inference Pass ===\")\n",
    "model.eval()\n",
    "model.reset_cache()\n",
    "\n",
    "input_ids = torch.randint(0, model_config['vocab_size'], (BATCH_SIZE, 1)).to(device)\n",
    "\n",
    "for i in range(5):\n",
    "    logits = model(input_ids, use_cache=True)\n",
    "    next_token_logits = logits[:, -1, :]\n",
    "    next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "    \n",
    "    print(f\"Step {i + 1}: Next token ID shape: {next_token_id.shape}\")\n",
    "\n",
    "    input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
    "\n",
    "print(\"\\nFinal generated sequence length:\", input_ids.shape[1])\n",
    "assert input_ids.shape[1] == 6, \"Generated sequence length mismatch\"\n",
    "print(\"Inference pass successful!\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
